# -*- coding: utf-8 -*-
"""
This module provides functionality for Attribution-Driven Credit Assignment (ADCA),
specifically the ADCA-GRPO method. It acts as an optional enhancement to the
standard advantage calculation in PPO, overwriting advantages based on semantic
step-level evaluation.
"""
from typing import Any, Tuple, Dict
from loguru import logger
import torch
from verl import DataProto

# Note: These imports are moved from the trainer's fit loop.
from agentevolver.utils.step_parser import verify_step_alignment, verify_step_content
from agentevolver.module.adv_processor.semantic_attribution import evaluate_step_flags_parallel_sync
from agentevolver.module.adv_processor.adca_grpo import (
    compute_prm_grpo_advantages, PRMHyper
)
from agentevolver.module.adv_processor.prompt import get_positive_mask


def apply_adca_grpo(
    batch: DataProto,
    attribution_cfg: Any,
    tokenizer: Any,
    global_steps: int,
    epoch: int,
    i: int,
) -> Tuple[DataProto, Dict[str, Any]]:
    """
    Applies Attribution-Driven Credit Assignment (ADCA-GRPO) logic.

    This function performs semantic evaluation of steps, calculates corresponding
    metrics, and, if enabled, overwrites the advantages in the batch with
    values computed from the PRM-GRPO scheme.

    Args:
        batch: The DataProto object containing the current batch data.
        attribution_cfg: The configuration object for attribution-driven credit assignment.
        tokenizer: The tokenizer instance.
        global_steps: The current global training step.
        epoch: The current training epoch.
        i: The index of the current batch within the epoch.

    Returns:
        A tuple containing:
        - The (potentially modified) DataProto object.
        - A dictionary of metrics generated by the ADCA process.
    """
    new_metrics = {}

    # --- Part A: Configuration Parsing and Condition Check ---
    enable_adca_grpo = getattr(attribution_cfg, 'enable', False)
    prm_cfg = getattr(attribution_cfg, "adca_grpo", None)
    enable_adca_metric = getattr(prm_cfg, 'enable_adca_metric', getattr(attribution_cfg, 'enable_adca_metric', False))
    prm_steps = getattr(prm_cfg, "prm_steps", 100)

    # If neither metrics nor advantage overwriting is enabled, exit early.
    if not (enable_adca_metric or enable_adca_grpo):
        return batch, new_metrics

    # --- Part B: Semantic Evaluation & Metrics Calculation ---
    # === (A) Parse and verify step boundaries ===
    if not verify_step_alignment(batch, tokenizer, global_steps):
        raise RuntimeError("Step alignment check failed!")


    for sample_idx in range(min(3, len(batch.batch["prompts"]))):
        verify_step_content(batch, tokenizer, sample_idx)

    # === (B) Evaluate all steps per sample via API ===
    flags, stats = evaluate_step_flags_parallel_sync(
        tokenizer=tokenizer,
        batch=batch,
        overall_score_source="token_level_rewards",
        mask_tensor=batch.batch["response_mask"],
        save_dir=getattr(attribution_cfg, 'llm_evaluation_log_dir', None),
        global_step=global_steps,
        epoch=f"train.{epoch}.{i}",
        skip_type=getattr(prm_cfg, 'skip_type', "skip_small_adv"),
        model_name=getattr(prm_cfg, 'model_name', "qwen-max"),
    )

    # --- PRM evaluation result statistics ---
    if isinstance(stats, dict):
        for k in (
            "prm/parse_success_rate", "prm/avg_steps_per_sample",
            "prm/p95_steps_per_sample", "prm/flags_len_mismatch_rate",
        ):
            v = stats.get(k, None)
            if v is not None:
                try:
                    new_metrics[k] = float(v)
                except Exception:
                    new_metrics[k] = v

    # --- Metrics: Consistency between PRM labels and ORM direction ---
    step_flags = flags if isinstance(flags, list) else flags.get("llm_parsed_flags", [])
    orm_sum = batch.batch.get("token_level_rewards", batch.batch["token_level_scores"]).sum(dim=-1)

    pos_mask = get_positive_mask(orm_sum)
    neg_mask = ~pos_mask

    def _count_for_indices(mask_tensor):
        total, good, bad = 0, 0, 0
        if mask_tensor.dtype != torch.bool:
            mask_tensor = mask_tensor.bool()
        idx_list = torch.nonzero(mask_tensor, as_tuple=False).view(-1).tolist()
        for idx in idx_list:
            if idx >= len(step_flags) or not step_flags[idx]:
                continue
            fs = step_flags[idx]
            total += len(fs)
            good += sum(1 for f in fs if f)
            bad += sum(1 for f in fs if not f)
        return total, good, bad

    pos_total, pos_good, pos_bad = _count_for_indices(pos_mask)
    neg_total, neg_good, neg_bad = _count_for_indices(neg_mask)

    new_metrics.update({
        "prm/pos_traj_bad_rate": (pos_bad / max(1, pos_total)),
        "prm/pos_traj_good_rate": (pos_good / max(1, pos_total)),
        "prm/neg_traj_good_rate": (neg_good / max(1, neg_total)),
        "prm/neg_traj_bad_rate": (neg_bad / max(1, neg_total)),
        "prm/good_steps_total": float(pos_good + neg_good),
        "prm/bad_steps_total": float(pos_bad + neg_bad),
    })

    
    # --- Part C: PRM-GRPO Advantage Overwriting ---
    if enable_adca_grpo and global_steps < prm_steps:
        # === (C0) Cosine decay alpha (no warmup) ===
        import math

        # 基础 α、下限与进度来源（优先显式 progress，其次 global_steps/total_steps）
        alpha0 = float(getattr(prm_cfg, "alpha", 0.1))
        use_cosine = bool(getattr(prm_cfg, "alpha_cosine_decay_enable", False))
        if use_cosine:
            alpha_min = float(getattr(prm_cfg, "alpha_min", 0.0))  # 可不配，默认 0

            if getattr(prm_cfg, "alpha_progress", None) is not None:
                p = float(getattr(prm_cfg, "alpha_progress"))
            else:
                total_steps = float(getattr(prm_cfg, "total_steps", 100))
                p = float(global_steps / total_steps) if total_steps > 0 else 0.0

            # clamp 到 [0,1]
            p = 0.0 if p < 0.0 else 1.0 if p > 1.0 else p

            # 纯余弦衰减（无 warmup）：alpha_t = alpha_min + 0.5*(alpha0 - alpha_min)*(1 + cos(pi*p))
            alpha_t = alpha_min + 0.5 * (alpha0 - alpha_min) * (1.0 + math.cos(math.pi * p))

            # 记录一下，便于监控
            new_metrics.update({
                "prm/alpha_base":     alpha0,
                "prm/alpha_t":        float(alpha_t),
                "prm/alpha_progress": float(p),
            })
        else:
            alpha_t = alpha0
        
        # === (C) PRM → GRPO Suffix Sum ===
        hyper = PRMHyper(
            consistent_scale=float(getattr(attribution_cfg, "consistent_scale", 1.0)),
            pos_unconsistent_scale=float(getattr(attribution_cfg, "pos_unconsistent_scale", 0.2)),
            neg_unconsistent_scale=abs(float(getattr(attribution_cfg, "neg_unconsistent_scale", 0.2))),
            do_batch_norm=bool(getattr(prm_cfg, "do_batch_norm", True)),
            equal_trajectory_weight=bool(getattr(prm_cfg, "equal_trajectory_weight", True)),
            fix_base=float(getattr(prm_cfg, "fix_base", 0.2)),
            alpha=alpha_t,
            orm_distribution=getattr(prm_cfg, "orm_distribution", "last_step"),
            enable_length_normalization=getattr(prm_cfg, "enable_length_normalization", False),
        )
        scheme = getattr(prm_cfg, "prm_scheme", "decouple")

        out = compute_prm_grpo_advantages(
            batch=batch,
            step_flags=flags if isinstance(flags, list) else flags["llm_parsed_flags"],
            hyper=hyper,
            scheme=scheme,
        )

        # Overwrite advantages for subsequent actor/critic updates
        batch.batch["advantages"] = out["advantages"]

        # Merge metrics from the decoupling scheme if they exist
        if isinstance(out, dict) and "metrics" in out and isinstance(out["metrics"], dict):
            new_metrics.update(out["metrics"])

    return batch, new_metrics
