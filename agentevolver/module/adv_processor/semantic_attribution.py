import torch
import verl.utils.torch_functional as verl_F
from openai import AsyncOpenAI, RateLimitError, APIError, BadRequestError
import os
import json
from pathlib import Path
from loguru import logger
import time
import traceback
from tqdm import tqdm
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict, Optional, Literal
import threading
from dataclasses import dataclass, asdict
import random
from agentevolver.module.adv_processor.prompt import build_batch_adv_evaluation_prompt, build_batch_reward_evaluation_prompt, get_positive_mask, THRESHOLD, rescale_score

__all__ = [
    "evaluate_step_flags_parallel",
    "ParallelSemanticProcessor",
]


@dataclass
class EvaluationTask:
    """Data structure for evaluation task - evaluates all steps of an entire sample at once"""
    sample_idx: int
    query: str
    rollout: str
    steps: List[Dict[str, str]]  # ‚Üê Originally List[str], unified to the structure of parse_rollout_to_steps
    overall_score: float

@dataclass
class EvaluationResult:
    """Data structure for evaluation result - contains all step results for an entire sample"""
    sample_idx: int
    step_results: List[bool]  # Evaluation results for all steps
    response_time: float

@dataclass
class EvaluationRecord:
    """Data structure for evaluation record, used for saving to file"""
    sample_idx: int
    query: str
    rollout: str
    steps: List[str]
    overall_score: float
    llm_input_messages: List[Dict]
    llm_raw_output: str
    llm_parsed_results: List[bool]  # Parsed results for all steps
    response_time: float
    timestamp: float
    model_name: str
    evaluation_type: str
    global_step: Optional[int] = None
    epoch: Optional[str] = None

# =========================================================
# Added: rollout parsing & batch-eval prompt utilities
# =========================================================
import re

def _steps_struct_to_text_list(steps: List[Dict[str, str]]) -> List[str]:
    """
    Converts a list of step dictionaries into a list of formatted strings.

    Args:
        steps (List[Dict[str, str]]): A list of dictionaries, each representing a step with 'action' and 'observation' keys.

    Returns:
        List[str]: A list of formatted strings, where each string represents a step.
    """
    out = []
    for st in steps:
        act = (st.get("action") or "").strip()
        obs = (st.get("observation") or "").strip()
        if obs:
            out.append(f"{act}\n\n[OBSERVATION]\n{obs}")
        else:
            out.append(act)
    return out




def parse_batch_evaluation_result(response: str, num_steps: int):
    """
    Parses the batch evaluation result from a given response string.

    Args:
        response (str): The response string containing the evaluation results.
        num_steps (int): The expected number of steps in the evaluation.

    Returns:
        List[bool]: A list of boolean values indicating whether each step is GOOD (True) or BAD (False).

    Raises:
        ValueError: If the evaluation result cannot be parsed to match the expected number of steps.
    """
    numbered = {}
    for m in re.finditer(r"Step\s+(\d+)\s+Judgment:\s*(GOOD|BAD)", response, flags=re.I):
        numbered[int(m.group(1))] = m.group(2).upper() == "GOOD"
    if len(numbered) == num_steps:
        return [numbered[i] for i in range(num_steps)]
    flags = re.findall(r"\b(GOOD|BAD)\b", response.upper())
    if len(flags) >= num_steps:
        return [flag == "GOOD" for flag in flags[:num_steps]]
    raise ValueError("Could not parse evaluation result")

def _get_overall_advantage(advantages_tensor, mask=None):
    """
    Extracts the overall advantage value from the given advantages tensor, optionally using a mask to filter elements.

    Args:
        advantages_tensor (torch.Tensor): The tensor containing advantage values.
        mask (torch.Tensor, optional): A boolean mask to filter the advantages tensor. Defaults to None.

    Returns:
        float: The overall advantage value, or 0.0 if no valid values are found.
    """
    if advantages_tensor.dim() == 0:
        return advantages_tensor.item()

    if advantages_tensor.dim() == 1:
        if mask is not None:
            valid_advantages = advantages_tensor[mask.bool()]
            if len(valid_advantages) > 0:
                return valid_advantages[0].item()
            else:
                return 0.0
        else:
            non_zero_mask = torch.abs(advantages_tensor) > 1e-8
            if non_zero_mask.any():
                return advantages_tensor[non_zero_mask][0].item()
            else:
                return 0.0

    raise ValueError(f"Unsupported advantages_tensor shape: {advantages_tensor.shape}")

def _save_evaluation_record(record: EvaluationRecord, save_dir: Optional[str] = None):
    """
    Saves an evaluation record to a file in a specified directory. The directory and filename are
    constructed based on the record's properties. Metadata is added to the record before saving.

    Args:
        record (EvaluationRecord): The evaluation record to be saved.
        save_dir (Optional[str]): The directory where the record will be saved. If None, the record is not saved.

    Returns:
        None
    """
    if save_dir is None:
        return

    try:
        base_save_path = Path(save_dir)
        base_save_path.mkdir(parents=True, exist_ok=True)

        if record.global_step is not None:
            step_subdir = f"step_{record.global_step:06d}"
        else:
            step_subdir = "step_unknown"

        step_save_path = base_save_path / step_subdir
        step_save_path.mkdir(parents=True, exist_ok=True)

        timestamp_str = f"{record.timestamp:.3f}".replace('.', '_')
        global_step_str = f"step{record.global_step:06d}" if record.global_step is not None else "nostep"
        filename = f"{global_step_str}_sample{record.sample_idx:03d}_{timestamp_str}.json"

        file_path = step_save_path / filename
        record_dict = asdict(record)
        record_dict["_metadata"] = {
            "save_time": time.time(),
            "step_directory": step_subdir,
            "file_name": filename,
            "full_path": str(file_path),
            "num_steps": len(record.steps)
        }

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(record_dict, f, ensure_ascii=False, indent=2)

        print(f"[record_save] ‚úÖ Saved sample {record.sample_idx} with {len(record.steps)} steps: {step_subdir}/{filename}")

    except Exception as e:
        print(f"[record_save] ‚ùå FAILED to save evaluation record for sample {record.sample_idx}: {e}")
        print(f"[record_save] üìÅ Path: {save_dir}")


async def _async_safe_query(
    client: AsyncOpenAI,
    model: str,
    messages: list[dict],
    semaphore: asyncio.Semaphore,
    max_retries: int = 200,
    timeout_s: int = 120,         # Adjustable: timeout threshold for single request
) -> str:
    """
    Asynchronously queries the LLM API with built-in retry logic for handling rate limits and other exceptions.
    Handles content moderation errors by aborting after 2 attempts.

    Args:
        client (AsyncOpenAI): The asynchronous OpenAI client.
        model (str): The name of the model to use for the query.
        messages (list[dict]): A list of message dictionaries to send to the model.
        semaphore (asyncio.Semaphore): A semaphore to control the number of concurrent requests.
        max_retries (int, optional): The maximum number of retries for the request. Defaults to 200.
        timeout_s (int, optional): The timeout in seconds for each request. Defaults to 120.

    Returns:
        str: The final response content from the model, or an empty string if content moderation fails twice.
    """
    async with semaphore:
        last_exception = None
        # üëá New addition: counter for tracking content moderation failures
        inappropriate_content_error_count = 0

        for attempt in range(max_retries):
            try:
                # ---------- Normal / thinking model branch (this part of the logic remains unchanged) ----------
                is_thinking_model = model.lower() in {
                    "qwq-plus",
                    "qwen3-30b-a3b-thinking-2507",
                    "qwen3-235b-a22b-thinking-2507",
                }

                if is_thinking_model:
                    print(f"[API] Using streaming mode for thinking model: {model}")
                    response = await client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=0.0,
                        extra_body={"enable_thinking": True},
                        stream=True,
                        max_tokens=8192,
                        timeout=timeout_s,
                    )

                    answer_content, reasoning_content = "", ""
                    async for chunk in response:
                        if not chunk.choices:
                            continue
                        delta = chunk.choices[0].delta
                        if getattr(delta, "reasoning_content", None):
                            reasoning_content += delta.reasoning_content
                        if getattr(delta, "content", ""):
                            answer_content += delta.content

                    final_answer = answer_content.strip()
                    return final_answer

                else:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=0.0,
                        timeout=timeout_s,
                        max_tokens=8192,
                    )
                    return response.choices[0].message.content.strip()

            # ---------- Unified exception handling (refactored logic) ----------
            except Exception as e:
                last_exception = e
                err_msg = str(e).lower()

                # üëá 1. Prioritize handling specific content moderation failure errors
                # Error code 'data_inspection_failed' or message contains 'inappropriate content'
                is_content_error = isinstance(e, BadRequestError) and (
                    "data_inspection_failed" in err_msg or "inappropriate content" in err_msg
                )
                if is_content_error:
                    inappropriate_content_error_count += 1
                    print(f"[API Warning] Content inspection failed (attempt {inappropriate_content_error_count}/2). Error: {e}")
                    if inappropriate_content_error_count >= 2:
                        print("[API Error] ‚ùå Content inspection failed twice. Aborting and returning empty string.")
                        return ""  # Condition met, directly return empty string and exit function

                # If not yet at maximum retries, decide how to wait based on error type
                if attempt < max_retries - 1:
                    # üëá 2. Handle rate limit errors (use elif to ensure logic independence)
                    is_rate_limit = any(
                        key in err_msg
                        for key in ["429", "rate limit", "exceeded", "limit_requests"]
                    )
                    if is_rate_limit:
                        backoff = min(1.5 ** attempt, 60)  # Upper limit 60s
                        jitter = backoff * 0.25 * random.random()
                        wait = backoff + jitter
                        print(f"[API Retry] Rate limit (attempt {attempt+1}/{max_retries}) "
                              f"sleep {wait:.1f}s")
                        await asyncio.sleep(wait)
                    else:
                        # üëá 3. Handle all other retryable exceptions (including first content moderation failure)
                        wait = min(2.0 * (attempt + 1), 15)
                        print(f"[API Retry] {type(e).__name__} (attempt {attempt+1}/{max_retries}) "
                              f"sleep {wait:.1f}s. Error: {e}")
                        await asyncio.sleep(wait)
                    
                    continue # Continue to next loop iteration
                
                # If maximum retries reached
                else:
                    print(f"[API Error] ‚ùå Max retries ({max_retries}) exceeded for error: {e}")
                    break # Break the for loop

        # If the loop completes normally or is interrupted by break (meaning all retries failed)
        # Note: If returning "" due to content moderation failure, code won't execute here
        print(f"[API Error] ‚ùå Failed after {max_retries} retries.")
        raise last_exception if last_exception else Exception("API query failed after all retries.")



async def _evaluate_single_sample_api(
    client: AsyncOpenAI,
    model_name: str,
    task: EvaluationTask,
    semaphore: asyncio.Semaphore,
    overall_score_source: str = "advantages",
    max_retries: int = 200,
    save_dir: Optional[str] = None,
    global_step: Optional[int] = None,
    epoch: Optional[str] = None
) -> EvaluationResult:
    """
    Evaluates a single sample using the API, including constructing prompts, calling the LLM, parsing results, and saving the evaluation record.

    Args:
        client (AsyncOpenAI): The asynchronous OpenAI client.
        model_name (str): The name of the model to use for evaluation.
        task (EvaluationTask): The task containing the sample to evaluate.
        semaphore (asyncio.Semaphore): Semaphore to control the number of concurrent API calls.
        overall_score_source (str, optional): The source for the overall score. Defaults to "advantages".
        max_retries (int, optional): Maximum number of retries for the API call. Defaults to 200.
        save_dir (Optional[str], optional): Directory to save the evaluation record. Defaults to None.
        global_step (Optional[int], optional): Global step for the evaluation. Defaults to None.
        epoch (Optional[str], optional): Epoch for the evaluation. Defaults to None.

    Returns:
        EvaluationResult: The result of the evaluation.
    """
    start_time = time.time()

    try:
        # 1) Construct batch evaluation prompt
        if overall_score_source == "token_level_rewards":
            messages = build_batch_reward_evaluation_prompt(
                task.query, task.steps, task.overall_score
            )
        elif overall_score_source == "advantages":
            messages = build_batch_adv_evaluation_prompt(
                task.query, task.steps, task.overall_score
            )

        # 2) Call the LLM
        llm_raw_output = await _async_safe_query(
            client, model_name, messages, semaphore, max_retries
        )

        # 3) Parse the results
        try:
            step_results = parse_batch_evaluation_result(
                llm_raw_output, len(task.steps)
            )
            print(
                f"[API] ‚úÖ Sample {task.sample_idx}: Successfully parsed "
                f"{len(step_results)} step results"
            )
        except Exception as parse_error:
            # ‚Äî‚Äî> Parsing failed: No rescaling (all use "no rescale" flag)
            print(
                f"[API] ‚ùå Sample {task.sample_idx}: Parse error, "
                f"disable rescale: {parse_error}"
            )
            uniform_flag = get_positive_mask(task.overall_score)
            step_results = [uniform_flag for _ in task.steps]

        response_time = time.time() - start_time

        # 4) Save the evaluation record
        if save_dir:
            is_thinking_model = model_name.lower() in {
                "qwq-plus",
                "qwen3-30b-a3b-thinking-2507",
                "qwen3-235b-a22b-thinking-2507",
            }
            record = EvaluationRecord(
                sample_idx=task.sample_idx,
                query=task.query,
                rollout=task.rollout,
                steps=_steps_struct_to_text_list(task.steps),  
                overall_score=task.overall_score,
                llm_input_messages=messages,
                llm_raw_output=llm_raw_output,
                llm_parsed_results=step_results,
                response_time=response_time,
                timestamp=time.time(),
                model_name=f"{model_name}{'_thinking' if is_thinking_model else ''}",
                evaluation_type="api",
                global_step=global_step,
                epoch=epoch,
            )
            _save_evaluation_record(record, save_dir)

        return EvaluationResult(
            sample_idx=task.sample_idx,
            step_results=step_results,
            response_time=response_time,
        )

    except Exception as e:
        # ‚Äî‚Äî> Overall API failure: No rescaling
        response_time = time.time() - start_time
        print(f"[parallel_eval] ‚ùå FAILED to evaluate sample {task.sample_idx}: {e}")

        uniform_flag = get_positive_mask(task.overall_score)
        step_results = [uniform_flag for _ in task.steps]

        if save_dir:
            record = EvaluationRecord(
                sample_idx=task.sample_idx,
                query=task.query,
                rollout=task.rollout,
                steps=task.steps,
                overall_score=task.overall_score,
                llm_input_messages=[],
                llm_raw_output=f"ERROR: {str(e)}",
                llm_parsed_results=step_results,
                response_time=response_time,
                timestamp=time.time(),
                model_name=model_name,
                evaluation_type="api",
                global_step=global_step,
                epoch=epoch,
            )
            _save_evaluation_record(record, save_dir)

        return EvaluationResult(
            sample_idx=task.sample_idx,
            step_results=step_results,
            response_time=response_time,
        )

async def evaluate_step_flags_parallel(tokenizer, batch, overall_score_source: str = "advantages", model_name: str = "qwen-max", evaluation_type: Literal["api"] = "api", max_concurrent: int = 20, batch_size_limit: int = 100, mask_tensor: torch.Tensor = None, api_max_retries: int = 200, save_dir: Optional[str] = None, global_step: Optional[int] = None, epoch: Optional[str] = None, skip_type: str='skip_small_adv') -> Tuple[List[List[bool]], Dict]:
    """
    Evaluates step flags in parallel for a batch of samples, with each sample being evaluated in one API call.

    Args:
        tokenizer (Tokenizer): The tokenizer used to decode prompts and responses.
        batch (Batch): The batch of samples to be evaluated.
        overall_score_source (str, optional): The source of the overall score, either "advantages" or "token_level_rewards". Defaults to "advantages".
        model_name (str, optional): The name of the model being used. Defaults to "qwen-max".
        evaluation_type (Literal["api"], optional): The type of evaluation, currently only "api" is supported. Defaults to "api".
        max_concurrent (int, optional): The maximum number of concurrent API calls. Defaults to 20.
        batch_size_limit (int, optional): The maximum size of the batch. Defaults to 100.
        mask_tensor (torch.Tensor, optional): An external mask tensor. Defaults to None.
        api_max_retries (int, optional): The maximum number of retries for API calls. Defaults to 200.
        save_dir (Optional[str], optional): The directory to save evaluation records. Defaults to None.
        global_step (Optional[int], optional): The global step in the training process. Defaults to None.
        epoch (Optional[str], optional): The current epoch. Defaults to None.
        skip_type (str, optional): The type of skipping logic to apply. Defaults to 'skip_small_adv'.

    Returns:
        Tuple[List[List[bool]], Dict]: A tuple containing the list of step flags for each sample and a dictionary with additional information.
    """
    batch_size = len(batch.batch['prompts'])
    print(f"[parallel_eval] Starting parallel evaluation for {batch_size} samples using API mode")
    print(f"[parallel_eval] üöÄ OPTIMIZED: One API call per sample (not per step)")
    print(f"[parallel_eval] Model: {model_name}, API max retries: {api_max_retries}")
    if save_dir:
        print(f"[parallel_eval] Saving evaluation records to: {save_dir}")

    if 'steps' not in batch.non_tensor_batch:
        raise ValueError("‚ùå batch.non_tensor_batch['steps'] is required but not found")

    if evaluation_type != "api":
        raise ValueError(f"‚ùå Only 'api' evaluation_type is supported, got: {evaluation_type}")

    # Initialize API client
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("‚ùå [parallel_eval] No API key found in DASHSCOPE_API_KEY environment variable")
        print("‚ùå [parallel_eval] Please set: export DASHSCOPE_API_KEY='your-api-key'")
        print("‚ùå [parallel_eval] Using random fallback for evaluation")
        # shuchang: 0809
        # FIXME: Comment out fallback, enforce API KEY requirement
        # return _apply_fallback_strategy_parallel(batch, tokenizer), {"fallback_used": True, "evaluation_type": evaluation_type}
        raise RuntimeError("No API key found in DASHSCOPE_API_KEY environment variable")

    api_client = AsyncOpenAI(
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )

    # üöÄ Key optimization: create tasks by sample, not by step
    all_tasks = []
    flags_per_sample = [[] for _ in range(batch_size)]
    skipped_samples = 0

    if mask_tensor is not None:
        response_mask = mask_tensor
        print(f"[parallel_eval] Using external mask tensor with shape {mask_tensor.shape}")

        response_length = batch.batch["responses"].size(1)
        if response_mask.shape != (batch_size, response_length):
            raise ValueError(f"‚ùå mask_tensor shape {response_mask.shape} doesn't match expected shape ({batch_size}, {response_length})")
    else:
        response_length = batch.batch["responses"].size(1)
        response_mask = batch.batch["loss_mask"][:, -response_length:]
        print(f"[parallel_eval] Using default loss_mask")

    for sample_idx in range(batch_size):
        query = tokenizer.decode(batch.batch["prompts"][sample_idx], skip_special_tokens=True)
        rollout = tokenizer.decode(batch.batch["responses"][sample_idx], skip_special_tokens=True)
        # shuchang: 0809
        # FIXME: Changed to use batch.non_tensor_batch["steps"] directly, no need for additional parsing
        # steps_struct = parse_rollout_to_steps(rollout)
        steps_struct = batch.non_tensor_batch["steps"][sample_idx]

        # mask and overall_score maintain original logic
        sample_mask = response_mask[sample_idx]
        advantage = _get_overall_advantage(batch.batch["advantages"][sample_idx], sample_mask)
        orm_reward = batch.batch["token_level_rewards"][sample_idx].sum().item()
        if overall_score_source == "token_level_rewards":
            # When using ORM, rescale reward based on THRESHOLD
            overall_score = rescale_score(orm_reward)
        elif overall_score_source == "advantages":
            # SSA mode: use calculated advantage
            overall_score = advantage
        else:
            overall_score = orm_reward
        # shuchang: 0906
        # Only skip samples with very small advantage or all negative samples
        # Decide whether to skip the current sample
        should_skip = False
        skip_reason = ""

        if skip_type == "skip_small_adv":
            # 1. Only skip samples with very small advantage
            if abs(advantage) < 1e-8:
                should_skip = True
                skip_reason = f"advantage‚âà0 ({advantage:.6f})"

        elif skip_type == "skip_all_neg":
            """
            Skips the evaluation of a sample if the ORM reward is non-positive.

            Args:
                orm_reward (float): The ORM reward value for the sample.
            """
            # 2. Skip samples with negative or zero orm_reward
            # Note: orm_reward > 0.5 is a positive sample, so <= 0.5 all belong to the "negative" category
            if orm_reward <= THRESHOLD:
                should_skip = True
                skip_reason = f"orm_reward is not positive ({orm_reward:.6f})"

        # If any skip condition is met, execute skip logic
        if should_skip:
            print(f"[parallel_eval] Sample {sample_idx}: Skipping evaluation due to {skip_reason}. Assigning flags based on overall_score.")
            # Decide flag value based on the sign of overall_score
            flag_value = overall_score > THRESHOLD
            flags_per_sample[sample_idx] = [flag_value] * len(steps_struct)

            if save_dir:
                record = EvaluationRecord(
                    sample_idx=sample_idx,
                    query=query,
                    rollout=rollout,
                    # ‚úÖ Still stored as List[str] in logs
                    steps=_steps_struct_to_text_list(steps_struct),
                    overall_score=overall_score,
                    llm_input_messages=[],
                    llm_raw_output="SKIPPED_ZERO_ADVANTAGE",
                    llm_parsed_results=[True] * len(steps_struct),
                    response_time=0.0,
                    timestamp=time.time(),
                    model_name=model_name,
                    evaluation_type=evaluation_type,
                    global_step=global_step,
                    epoch=epoch
                )
                _save_evaluation_record(record, save_dir)
            skipped_samples += 1
            continue


       # ‚úÖ EvaluationTask uses structured steps
        task = EvaluationTask(
            sample_idx=sample_idx,
            query=query,
            rollout=rollout,
            steps=steps_struct,
            overall_score=overall_score
        )
        all_tasks.append(task)

    total_tasks = len(all_tasks)
    total_api_calls = total_tasks  # Now each sample only needs one API call
    total_steps = sum(len(t.steps) for t in all_tasks)
    # --- Metric preparation: step length for each sample ---
    step_len_map = {t.sample_idx: len(t.steps) for t in all_tasks}
    step_len_list = list(step_len_map.values())

    print(f"[parallel_eval] üöÄ EFFICIENCY GAIN:")
    print(f"[parallel_eval]   - Total samples: {batch_size}")
    print(f"[parallel_eval]   - Total steps: {total_steps}")
    print(f"[parallel_eval]   - API calls needed: {total_api_calls} (instead of {total_steps})")
    print(f"[parallel_eval]   - Efficiency gain: {total_steps/max(1,total_api_calls):.1f}x fewer API calls")
    print(f"[parallel_eval]   - Skipped {skipped_samples} samples with advantage=0")

    if total_tasks == 0:
        print("[parallel_eval] No tasks to process, all samples had advantage=0")
        await api_client.close()
        return flags_per_sample, {
            "total_tasks": 0,
            "total_api_calls": 0,
            "total_steps": total_steps,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_api_time": 0,
            "avg_api_time": 0,
            "max_concurrent": max_concurrent,
            "fallback_used": False,
            "skipped_samples": skipped_samples,
            "evaluation_type": evaluation_type,
            "api_max_retries": api_max_retries,
            "efficiency_gain": 0
        }

    all_results = []
    semaphore = asyncio.Semaphore(max_concurrent)

    with tqdm(total=total_tasks, desc=f"[parallel_eval] Processing samples (API)") as pbar:
        for i in range(0, total_tasks, batch_size_limit):
            batch_tasks = all_tasks[i:i + batch_size_limit]

            # Each task calls _evaluate_single_sample_api to evaluate all steps of the entire sample at once
            coroutines = [
                _evaluate_single_sample_api(api_client, model_name, task, semaphore, overall_score_source, api_max_retries, save_dir, global_step, epoch)
                for task in batch_tasks
            ]
            batch_results = await asyncio.gather(*coroutines, return_exceptions=True)

            for result in batch_results:
                if isinstance(result, Exception):
                    print(f"[parallel_eval] ‚ùå Task failed with exception: {result}")
                    continue
                all_results.append(result)

            pbar.update(len(batch_tasks))

    # Organize results into flags_per_sample
    for result in all_results:
        flags_per_sample[result.sample_idx] = result.step_results

    total_time = sum(r.response_time for r in all_results)
    avg_time = total_time / len(all_results) if all_results else 0

    stats = {
        "total_tasks": total_tasks,
        "total_api_calls": len(all_results),
        "total_steps": total_steps,
        "successful_tasks": len(all_results),
        "failed_tasks": total_tasks - len(all_results),
        "total_api_time": total_time,
        "avg_api_time": avg_time,
        "max_concurrent": max_concurrent,
        "fallback_used": False,
        "skipped_samples": skipped_samples,
        "evaluation_type": evaluation_type,
        "model_name": model_name,
        "api_max_retries": api_max_retries,
        "save_dir": save_dir,
        "efficiency_gain": total_steps / max(1, len(all_results))  # Efficiency gain multiplier
    }
    def _p95(vals):
        """
        Calculates the 95th percentile of a list of values.

        Args:
            vals (list): A list of numeric values.

        Returns:
            float: The 95th percentile value.
        """
        if not vals:
            return 0.0
        s = sorted(vals)
        k = int(round(0.95 * (len(s) - 1)))
        return float(s[k])

    parsed_ok = sum(
        1 for r in all_results
        if len(r.step_results) == step_len_map.get(r.sample_idx, 0)
    )
    length_mismatch = sum(
        1 for r in all_results
        if len(r.step_results) != step_len_map.get(r.sample_idx, 0)
    )

    stats.update({
        "prm/parse_success_rate": parsed_ok / max(1, total_tasks),
        "prm/avg_steps_per_sample": (sum(step_len_list) / max(1, len(step_len_list))) if step_len_list else 0.0,
        "prm/p95_steps_per_sample": _p95(step_len_list),
        "prm/flags_len_mismatch_rate": length_mismatch / max(1, total_tasks),
        "prm/_parse_success_count": parsed_ok,
        "prm/_flags_len_mismatch_count": length_mismatch,
    })

    print(f"[parallel_eval] ‚úÖ Completed with {stats['efficiency_gain']:.1f}x efficiency gain!")
    print(f"[parallel_eval] Stats: {stats}")

    await api_client.close()
    return flags_per_sample, stats


def evaluate_step_flags_parallel_sync(tokenizer, batch, **kwargs):
    """
    Synchronous wrapper for the `evaluate_step_flags_parallel` function.

    This function allows the `evaluate_step_flags_parallel` function to be called in a synchronous context by managing
    the asyncio event loop and running the asynchronous function until completion.

    Args:
        tokenizer: The tokenizer used for processing text.
        batch: The batch of data to be evaluated.
        **kwargs: Additional keyword arguments to be passed to the `evaluate_step_flags_parallel` function.

    Returns:
        The result of the `evaluate_step_flags_parallel` function.
    """
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(
        evaluate_step_flags_parallel(tokenizer, batch, **kwargs)
    )