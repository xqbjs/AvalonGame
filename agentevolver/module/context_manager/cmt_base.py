from agentevolver.schema.trajectory import Reward, Trajectory
from typing import List, Dict
import uuid as uuid_gen


class ContextManagerBase:
    """
    Base class for context management, providing a framework for handling and processing messages, including tokenization, content clipping, and loss masking.

    Methods:
        - save_init_input: Saves the initial input.
        - prepare_next_llm_context: Prepares the next context for the LLM.
        - check_context_token_num_safe: Checks if the number of tokens in the context is safe.
        - prepare_world_interaction: Prepares the interaction with the world.
        - save_llm_output: Saves the output from the LLM.
        - save_env_output: Saves the output from the environment.
        - remove_last_context: Removes the last context.
        - generate_log: Generates a log.
        - group_tokenize: Tokenizes a group of messages.
    """

    def save_init_input(self, init_input_arr: List):
        """
        Saves the initial input array. This method should be implemented by subclasses.

        Args:
            init_input_arr (List): The initial input array to be saved.

        Raises:
            NotImplementedError: This method needs to be implemented by a subclass.
        """
        raise NotImplementedError

    def prepare_next_llm_context(self, **kwargs) -> List:
        """
        Prepares the next context for the LLM. This method should be implemented by subclasses.

        Args:
            **kwargs: Arbitrary keyword arguments that may be required for context preparation.

        Returns:
            List: The prepared context for the LLM.

        Raises:
            NotImplementedError: This method needs to be implemented by a subclass.
        """
        raise NotImplementedError

    def prepare_world_interaction(self, **kwargs) -> str:
        """
        Prepares the world interaction. This method should be overridden by subclasses to provide specific implementation.

        Args:
            **kwargs: Arbitrary keyword arguments that can be used by subclasses to customize the preparation of the world interaction.

        Returns:
            str: A string indicating the result or status of the preparation.
        """
        raise NotImplementedError

    def save_llm_output(self, llm_output, **kwargs):
        """
        Saves the output from a language model. This method must be implemented by subclasses.

        Args:
            llm_output: The output generated by the language model.
            **kwargs: Additional keyword arguments that may be used by subclasses.

        Raises:
            NotImplementedError: If the method is not overridden by a subclass.
        """
        raise NotImplementedError

    def save_env_output(self, env_output, **kwargs):
        """
        Saves the output of the environment. This method should be implemented by subclasses.

        Args:
            env_output: The output from the environment that needs to be saved.
            **kwargs: Additional keyword arguments that may be required for saving the output.

        Raises:
            NotImplementedError: If the method is not overridden by a subclass.
        """
        raise NotImplementedError

    def group_tokenize(self):
        """
        Placeholder method for tokenizing a group of trajectories. This method should be implemented in subclasses.

        Raises:
            NotImplementedError: If the method is not overridden by a subclass.
        """
        raise NotImplementedError



class ExtendedMessage:

    def __init__(
            self,
            author,
            role="assistant",
            content="",
            token_arr=[],
            token_begin_index=-1,
            token_end_index=-1,
            clip=False,
            clip_token_limit=8192,
            tokenizer=None,
            token_generator="manual",
            build_from_uuid="",
            uuid=None,
        ):
        """
        Initializes an ExtendedMessage object with various attributes related to the message and its tokens.

        Args:
            author (str): The author of the message.
            role (str, optional): The role of the message sender. Defaults to "assistant".
            content (str, optional): The content of the message. Defaults to "".
            token_arr (list, optional): An array of tokens. Defaults to [].
            token_begin_index (int, optional): The starting index of the tokens. Defaults to -1.
            token_end_index (int, optional): The ending index of the tokens. Defaults to -1.
            clip (bool, optional): Whether to clip the content. Defaults to False.
            clip_token_limit (int, optional): The maximum number of tokens allowed when clipping. Defaults to 8192.
            tokenizer (Tokenizer, optional): The tokenizer to use for tokenization. Defaults to None.
            token_generator (str, optional): The method used to generate tokens. Defaults to "manual".
            build_from_uuid (str, optional): The UUID from which the message is built. Defaults to "".
            uuid (str, optional): The unique identifier for the message. Defaults to None.
        """
        self.author = author
        self.role = role
        self.content = content
        self.token_arr = token_arr
        self.token_begin_index = token_begin_index
        self.token_end_index = token_end_index
        # use property to ensure content is safe before use
        self._content_for_future = ""
        self._info = ""
        self.clip = clip
        if uuid is None:
            self.uuid = uuid_gen.uuid4().hex
        else:
            self.uuid = uuid
        self.build_from_uuid = build_from_uuid

        if not clip:
            self.generate_content_for_future(tokenizer=None, clip=False)
        else:
            self.generate_content_for_future(tokenizer=tokenizer, clip=True, clip_token_limit=clip_token_limit)  # ⭐ Generates future content with or without clipping
        self.eos_token_id = tokenizer.eos_token_id
        if token_generator == 'auto':
            dummy_msg = [ {"role": "assistant",  "content": "dummy text"} ]
            self.token_arr, _ = self.get_inc_simple(
               text_frag_from=tokenizer.apply_chat_template(dummy_msg, tokenize=False),
               text_frag_to=tokenizer.apply_chat_template(dummy_msg +
                    [ {"role": self.role,  "content": self.content_for_future} ], tokenize=False),
               tokenizer=tokenizer
            )  # ⭐ Automatically generates tokens for the message


    @property
    def content_for_future(self):
        """
        Property that returns the content for future use. If the content is empty, it sets a default value.

        Returns:
            str: The content for future use.
        """
        if self._content_for_future == "":
            self._content_for_future = "(Empty Content)"
            # raise ValueError("content_for_future is not set, or previous llm output is empty!")
        return self._content_for_future  # ⭐ Ensures that the content is not empty


    @property
    def need_training(self):
        """
        Property that determines whether the message needs training based on the author.

        Returns:
            bool: True if the message needs training, False otherwise.
        """
        NEED_TRAIN_AUTHORS = ["llm"]
        NON_TRAIN_AUTHORS = ["env", "initialization", "user", "memory", "llm(do_not_train)"]
        assert (self.author in NEED_TRAIN_AUTHORS) or (self.author in NON_TRAIN_AUTHORS) or (self.author.endswith('(discard)')), f"author {self.author} is not identified"
        return (self.author in NEED_TRAIN_AUTHORS)  # ⭐ Determines if the message needs training based on the author


    def generate_content_for_future(self, tokenizer, clip, clip_token_limit=-1):
        """
        Generates a version of the content that is suitable for future use, potentially clipping it to fit within a specified token limit.

        Args:
            tokenizer (Tokenizer): The tokenizer used to count the number of tokens in the content.
            clip (bool): A flag indicating whether to clip the content if it exceeds the token limit.
            clip_token_limit (int, optional): The maximum number of tokens allowed. Defaults to -1, which means no clipping.

        Returns:
            None: The result is stored in the `_content_for_future` attribute.
        """
        _content: str = self.content
        if clip:
            assert clip_token_limit > 0, "clip_token_limit must be set when clip is True"
            n_token = len(tokenizer(_content, return_tensors="pt", padding=False)["input_ids"][0])  # ⭐ Count the number of tokens in the content
            if n_token > clip_token_limit:
                n_char = len(_content)  # 10,000
                eps = 100   # token
                preserve_percent = (clip_token_limit - eps) / n_token  # 3900 / 8000
                n_char_to_preserve = int(n_char * preserve_percent)
                _content = _content[:n_char_to_preserve] + "... truncate ..."  # ⭐ Truncate the content and add an ellipsis
        self._content_for_future = _content


    def get_loss_mask(self, blackout_token_combo):
        """
        Generates a loss mask for the token array, blacking out specific token combinations and everything after the EOS token.

        Args:
            blackout_token_combo (list): A list of token IDs that should be blacked out in the first encounter.

        Returns:
            list: A binary mask where 1 indicates the token contributes to the loss, and 0 indicates it does not.
        """
        def blackout_specific_token_ids_first_encounter(mask, arr, token_ids):
            """
            Blackouts the first occurrence of a specific sequence of token IDs in the mask.

            Args:
                mask (list): The initial mask.
                arr (list): The token array.
                token_ids (list): The sequence of token IDs to black out.

            Returns:
                list: The updated mask.
            """
            index = find_sublist_indices(arr, token_ids, reverse=False)
            if index >= 0:
                for i in range(index, index+len(token_ids)): mask[i] = 0  # ⭐ Blackout the specific token IDs
            return mask

        def blackout_everything_after_eos_but_keep_eos(mask, token_arr, eos_token_id):
            """
            Blackouts everything after the EOS token, but keeps the EOS token itself.

            Args:
                mask (list): The initial mask.
                token_arr (list): The token array.
                eos_token_id (int): The EOS token ID.

            Returns:
                list: The updated mask.
            """
            eos_position = token_arr.index(eos_token_id) if eos_token_id in token_arr else -1
            if eos_position != -1:
                for i in range(eos_position + 1, len(mask)):
                    mask[i] = 0  # ⭐ Blackout everything after the EOS token
            return mask

        if self.need_training:
            msg_token_mask = [1] * len(self.token_arr)
            msg_token_mask = blackout_specific_token_ids_first_encounter(msg_token_mask, self.token_arr, blackout_token_combo)
            msg_token_mask = blackout_everything_after_eos_but_keep_eos(mask=msg_token_mask, token_arr=self.token_arr, eos_token_id=self.eos_token_id)
            return msg_token_mask
        else:
            msg_token_mask = [0] * len(self.token_arr)
            return msg_token_mask

    def get_inc_simple(self, text_frag_from, text_frag_to, tokenizer):
        """
        Get the incremental token array from `text_frag_from` to `text_frag_to`.

        Args:
            text_frag_from (str): The original text fragment.
            text_frag_to (str): The target text fragment.
            tokenizer: The tokenizer used to convert text into tokens.

        Returns:
            tuple: A tuple containing the list of incremental token IDs and a message string.
        """
        tokenizer_output = tokenizer(text_frag_from, return_tensors="pt", padding=False)
        tokenizer_input_ids = tokenizer_output["input_ids"][0].tolist()
        token_ids_acc = tokenizer_input_ids  # ⭐ Accumulate the token IDs from the original text

        tokenizer_output = tokenizer(text_frag_to, return_tensors="pt", padding=False)
        input_ids = tokenizer_output["input_ids"][0].tolist()
        input_id_increment = input_ids[len(token_ids_acc):]  # ⭐ Get the new tokens added in this step
        overlap_length = 0
        for i in range(len(token_ids_acc)):
            if i < len(token_ids_acc) and input_ids[i] == token_ids_acc[i]: overlap_length += 1
            else: break
        msg = f"previous token length: {len(token_ids_acc)}, overlap token length: {(overlap_length)}, increment token length: {len(input_id_increment)}"
        # print(msg)
        return input_id_increment, msg


def find_sublist_indices(large_list, small_list, reverse=False):
    """
    Finds the starting index of the first occurrence of `small_list` in `large_list`.
    If `reverse` is True, the search is conducted from the end of `large_list`.

    Args:
        large_list (list): The list in which to search for the `small_list`.
        small_list (list): The sublist to search for within `large_list`.
        reverse (bool, optional): If True, the search starts from the end of `large_list`. Defaults to False.

    Returns:
        int: The starting index of the first occurrence of `small_list` in `large_list`, or -1 if not found.
    """
    small_len = len(small_list)
    if reverse:
        for i in reversed(range(len(large_list) - small_len + 1)):
            if large_list[i: i+small_len] == small_list:  # ⭐ Check if the current slice matches the small_list
                return i
    for i in range(len(large_list) - small_len + 1):
        if large_list[i: i+small_len] == small_list:  # ⭐ Check if the current slice matches the small_list
            return i
    return -1


def replace_token_ids(place_holder, replace_with, begin, end):
    """
    Replaces a segment of `place_holder` identified by `begin` and `end` markers with `replace_with`.
    Ensures that `begin` and `end` tokens are not included in the final result if they were part of `replace_with`.

    Args:
        place_holder (list): The original list where the replacement will occur.
        replace_with (list): The list to be inserted in place of the segment between `begin` and `end`.
        begin (list): The marker indicating the start of the segment to be replaced.
        end (list): The marker indicating the end of the segment to be replaced.

    Returns:
        list: The modified list after the replacement.
    """
    _begin_index = find_sublist_indices(place_holder, begin) + len(begin)  # ⭐ Find the index right after the `begin` marker
    _end_index = find_sublist_indices(place_holder, end, reverse=True)  # ⭐ Find the index at the `end` marker

    if replace_with[-len(end):] == end:  # remove end token
        replace_with = replace_with[:-len(end)]
    if replace_with[:len(begin)] == begin:  # remove begin token
        replace_with = replace_with[len(begin):]

    final = place_holder[:_begin_index] + replace_with + place_holder[_end_index:]  # ⭐ Construct the final list with the replacement
    return final
